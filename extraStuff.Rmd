---
title: "Respose to Hoijtink et al., extras"
author: "Richard D. Morey"
date: "4 February 2015"
output: html_document
---

This document contains some extra content that is not in the manuscript of our rejoinder, including some details about the code and reproduction of the figures in Hoijtink et al. using my code rather than the sampling routines, and also some proofs.

All code can be found in the file `utility.R`.
```{r}
## for some convenience functions
source('utility.R')
```

The code in `utility.R` uses the `BayesFactor` package to do some arithmetic with logarithms (there's probably a better way to do it in the general case, but the algorithms converge for the ranges of values we need).

The most important functions are:

*  `calibrated_tau(d1, N, k = 10, NRshift = .1, tol = .01)`: Find $\tau$ such that the error rates are equal
    *  `d1`: true effect size $\delta_1$ against which to calibrate
    *  `N`: sample size
    *  `k`: number of iterations for Newton-Raphson solver
    *  `NRshift`: distance from minimum at which to start the Newton-Raphson solver
    *  `tol`: tolerance, in proportion, on the solution. The solution should yield a Bayes factor of 1 at a known t statistic. if it isn't within the `tol` of 1, then a warning will be issued.
*  `probs(tau, d1, N, k = 4)`: Find the critical $t$ for a given (log) Bayes factor to be equal to 0, and report errors under $H_0$ and calibrated $\delta=\delta_1$
    *  `tau`: scaled-information Bayes factor prior scale under $H_1$
    *  `d1`: true effect size $\delta_1$ against which to calibrate
    *  `N`: sample size
    *  `k`: number of iterations for Newton-Raphson solver

### Reproduce plots in Hoijtink et al.

We first use the functions to reproduce the plots in Hoijtink et al., to show that the code yields the same numbers as Hoijtink's code, but without requiring sampling (and nearly instantaneously).

```{r echo=FALSE}
##  Reproduce their Figure 1

## Settings for figure
N = 100
d1 = 0:4 * .05 + .2
tau = seq(.1, 1, len=20)

## Compute probabilities for all combinations of the settings
setup = expand.grid(d1 = d1, tau = tau)
q = mapply(probs, tau = setup$tau, d1 = setup$d1, MoreArgs = list(N=N))

## Prepare values for matplot 
pnull = q[2,]
palt = q[3,]
dim(palt) = dim(pnull) = c(length(d1),length(tau))

## Create figure
matplot(tau, t(palt), typ='l', ylim = c(.4, 1), ylab="P(.|.)")
lines(tau, pnull[1,])
abline(v=c(.125,.225,.9),lty=2)

## Add points at intersection of lines using calibrated_tau()
cd = crit_delta(d1,N)
points(calibrated_tau(d1,N),1-2*pt(-cd*sqrt(N),N-1))

```

We can also reproduce their Figure 2.

```{r echo=FALSE}
## Reproduce their Figure 2

## Settings for figure
N = 36
d1 = c(.20, .25, .30, .35, .40, .50, .60)
tau = seq(.1, 1, len=20)

## Compute probabilities for all combinations of the settings
setup = expand.grid(d1 = d1, tau = tau)
q = mapply(probs, tau = setup$tau, d1 = setup$d1, MoreArgs = list(N=N))

## Prepare values for matplot 
pnull = q[2,]
palt = q[3,]
dim(palt) = dim(pnull) = c(length(d1),length(tau))

## Create figure
matplot(tau, t(palt), typ='l', ylim = c(.4, 1), ylab="P(.|.)")
lines(tau, pnull[1,])
abline(v=c(.33,1),lty=2)

## Add points at intersection of lines using calibrated_tau()
cd = crit_delta(d1,N)
points(calibrated_tau(d1,N),1-2*pt(-cd*sqrt(N),N-1))

```

The algorithm fails to converge for one of the values off the plot to the left. This is because there is *no (nontrivial) solution* for $\delta_1=.2$ (oddly enough, this value is cut off of the left side of Figure 2 in HKH). The lack of nontrivial solution will be proved in the next section.

### Proof of nonexistance of nontrivial calibration when critical $|t|\leq1$

The logarithm $B$ of the scaled information Bayes factor in favor of the null is
\[
B =   -\frac{N}{2}\log\left(1 + \frac{t^2}{N-1}\right) +  \frac{N}{2}\log\left(1 + \frac{t^2}{ (1 + N\tau^2)( N - 1 ) } \right)  + \frac{1}{2}\log\left(1 + N\tau^2\right)
\]
This will be a decrase
Both definitions of calibration suggested by HKH involve finding a $\tau$ such that $B=0$ for a given critical $t$. A nontrivial HKH-calibration only exists when there is a $\tau>0$ such that $B=0$. If the solution is $\tau=0$, then the Bayes factor will be 1 regardless of the data, because the null and alternative are the same. This can be verified by setting $\tau=0$ in the equation for the scaled-information Bayes factor formula above.

It is also trivial to show that
\[
\lim_{\tau\rightarrow\infty} B = \infty;
\]
that is, as $\tau$ gets larger the Bayes factor approaches $\infty$. The null will be favored as the alternative places weight on arbitrarily high effect sizes. 

The figure below shows both of these trends for the critical $t$ statistic for an HKH-calibration against $\delta_1=.2$, $\delta_1=.25$, and $\delta_1=.3$ with a sample size of $N=36$ (all of these values can be found in HKH Figure 2).


```{r echo=FALSE}

par(las=1, cex=1.2)
tt = seq(0,.2, len=500)

N = 36
d1 = c(.20,.25,.3)
t0 = crit_delta(d1 = d1, N = N) * sqrt(N)

setup = expand.grid(t0 = t0, tau = tt)
bf = mapply(logBF, t = setup$t0, tau = setup$tau, MoreArgs = list(N=N))
dim(bf) = c(length(d1), length(tt))

matplot(tt,t(exp(bf)),typ='l',ylab="Bayes factor",xlab=expression(tau),lwd=2,log="y")
abline(h=1,col="gray")

legend(0,1.2,legend=c(expression(delta[1]==.2),expression(delta[1]==.25),expression(delta[1]==.3)),lwd=2,lty=1:3,col=1:3)

points(calibrated_tau(d1[2:3], N),c(1,1))

```

If there is a nontrivial solution for $B=0$, then the Bayes factor must drop below $B=0$ before rising back above $B=0$, and therefore must have a minimum. At this minimum the derivative of the log Bayes factor $B$ will be 0. 

Taking the derivative of $B$ with respect to $q = 1 + N\tau^2$ and setting equal to 0 yields
\[
\frac{dB}{dq} = \frac{1}{2q}\left(1 - \frac{Nt^2}{(N-1)q + t^2}\right) = 0
\]

Solving this for $q$ yields
\[
q = t^2
\]
and since $q = 1 + N\tau^2$,
\[
\tau = \sqrt{\frac{t^2-1}{N}}
\]
For this to yield a nontrivial solution $\tau>0$,
\[
\frac{t^2-1}{N}>0
\]
which implies that $|t|>1$. The critical $t$ statistic under HKH-calibration which must be set to yield $B=0$ must in turn be greater than 1 (or less than -1), or there will be no solution such that $\tau>0$ yields a Bayes factor of 1.

For the values of $\delta_1$ in the figure above (.2,.25, and .3), the critical $|t|$ statistics under HKH-calibration Definition 1 are

$\delta_1$ | critical $|t|$ | HKH-$\tau$
-----------|--------------|----
0.2        | `r t0[1]`    | 0 (No nontrivial solution) 
0.25       | `r t0[2]`    | `r calibrated_tau(d1[2], N)`
0.3        | `r t0[3]`    | `r calibrated_tau(d1[3], N)`

For $\delta_1=.2$ and $N=36$, HKH-calibration will *always* yield $BF=1$. The shaded region in the figure below shows the combinations of $\delta_1$ and $N$ that will have no nontrivial calibration. The vertical lines show the sample sizes used in HKH Figures 1 and 2; the three points represent the effect sizes shown above.

```{r echo=FALSE}

par(las=1, cex=1.2,lwd=2)

N = 3:700
faild = find_d1(1,N)

plot(log(N),faild,ylim=c(0,.6), xlim=log(c(4,500)), ylab=expression(paste("Calibrated ",delta[1])), xlab="N (log scale)",typ='n',xaxt='n')
abline(v=log(2.5), col="gray")
abline(h=0, col="gray")

axis(1,at = log(c(5,10,25,50,100,200,500)), lab=c(5,10,25,50,100,200,500))
polygon(c(rev(log(N)),log(N)),c(0*faild,faild),col=rgb(1,0,0,.2))

text(log(5),.2, expression(tau==0), adj=0)
text(log(50),.5, expression(tau>0), adj=0)

abline(v = log(c(36,100)), lty=2,col="gray")
points(rep(log(36),3), seq(.2,.3,.05), pch=21, col="black", bg="black")



```

For $\delta_1=.25$ and $N=36$, HKH-calibration yields a $\tau$ value so small that *no matter how large the effect size*, the Bayes factor can never exceed 7.1 in favor of the alternative.

```{r echo=FALSE}
N = 36
d1 = c(.20,.25,.3)
```

### Proof

Taking the limit as $t\rightarrow\infty$,
\[
\lim_{t\rightarrow\infty} B = -\frac{N-1}{2}\log\left(1 + N\tau^2\right)
\]
which for $\tau=`r calibrated_tau(d1[2], N)`$, will be `r -(N-1)/2*log(1 + N*calibrated_tau(d1[2], N)^2)`. Exponentiating yields 1/`r exp((N-1)/2*log(1 + N*calibrated_tau(d1[2], N)^2))`.

The plot below shows this limiting behaviour.
```{r echo=FALSE}


tt = 0:200

plot(tt,exp(-logBF(tt,N,calibrated_tau(d1[2], N))), t='l',ylab="Bayes factor (for H1)",xlab="Observed t statistic", log="y")

abline(h = exp((N-1)/2*log(1 + N*calibrated_tau(d1[2], N)^2)), col="red")

```

It should be noted that this limiting behaviour for the scaled-information Bayes factor is known (see Liang et al. 2008, who call it the *information paradox*; the existence of this limit has been used as an argument for the JZS prior) but for most reasonable analysis, this limit is very large. For HKH-calibration, however, the very small size of $\tau$ makes the limit a much more modest size.

### Proof that criteria approach $\delta_1/2$ as $N\rightarrow\infty$

Here we prove that as $N\rightarrow\infty$, the critical $d$ for the equal-error two-sided significance test approaches $\delta_1/2$ described in the manuscript. 

First, we note that the sampling distribution of $d$ is approximately normal for moderate to large $N$, with the approximation improving as $N\rightarrow\infty$:
\[
d\stackrel{approx.}{\sim}\mbox{Normal}(\delta,1/N).
\]
Assuming this approximation, let $\pm z_n$ be the critera for an equal-error sided significance test against $\delta=\delta_1$ when $N=n$. This means that
\[
1-\Phi(z_n\sqrt{N}) = \Phi(-z_n\sqrt{N}) = \alpha_n/2,
\]
where $\Phi$ is the CDF of the standard normal distribution, and $\alpha_n$ is the Type I error rate when $N=n$; and
\[
\Phi\left(\left(z_n - \delta_1\right)\sqrt{N}\right) - \Phi\left(\left(-z_n - \delta_1\right)\sqrt{N}\right) = \beta_n.
\]
where $\beta_n$ is the Type II error rate when $N=n$. Since we assume equal errors, $\alpha_n = \beta_n$, and thus
\[
\begin{eqnarray*}
1 &=& \beta_n/\alpha_n\\ 
  &=& \frac{\Phi\left(\left(z_n - \delta_1\right)\sqrt{N}\right) - \Phi\left(\left(-z_n - \delta_1\right)\sqrt{N}\right)}{2\Phi(-z_n\sqrt{N})}\\
  &=&\frac{\Phi\left(\left(z_n - \delta_1\right)\sqrt{N}\right)}{2\Phi(-z_n\sqrt{N})} - \frac{\Phi\left(\left(-z_n - \delta_1\right)\sqrt{N}\right)}{2\Phi(-z_n\sqrt{N})}
\end{eqnarray*}
\]
The second term on the right-hand side --- the probability of observing a "significant" observation on the wrong side of 0, given $\delta=\delta_1$ --- will quickly diminish to 0 as $N\rightarrow\infty$. If we denote this term by $q_n>0$, some algebra yields
\[
\begin{eqnarray*}
(1)~~~~~~~~2(1+q_n)\Phi(-z_n\sqrt{N})&=&\Phi\left(\left(z_n - \delta_1\right)\sqrt{N}\right).
\end{eqnarray*}
\]
The left-hand side of (1) is decreasing in both $z_n$ and $N$, while the right side is increasing in $z_n$ and $N$. Therefore, for the equality to hold while $N$ increases, $z_n$ must decrease:
\[
(2)~~~~~~~~z_{n+1}<z_n.
\]
We also note that, by (1),
\[
\begin{eqnarray*}
\Phi(-z_n\sqrt{N})&<&\Phi\left(\left(z_n - \delta_1\right)\sqrt{N}\right)\\
\end{eqnarray*}
\]
which implies that 
$$
(3)~~~~~~~~z_n>\delta_1/2.
$$
Facts (2) and (3) jointly imply that $z_n$ must have a finite limit as $N\rightarrow\infty$, and that limit must be at least $\delta_1/2$.

If we let $z_n=\delta_1/2+\epsilon_n$, we know from (2) that $\epsilon_n>0$, and therefore
\[
(4)~~~~~~~~\lim_{N\rightarrow\infty}\frac{\Phi\left(-\left(\frac{\delta_1}{2} - \epsilon_n\right)\sqrt{N}\right)}{\Phi(-\left(\frac{\delta_1}{2}+\epsilon_n\right)\sqrt{N})} = \infty.
\]
if $\lim_{N\rightarrow\infty}\epsilon_n\neq0$. But this limit must be finite for the equality in (1) to hold as $N\rightarrow\infty$.

Therefore, by (2), (3), and (4), 
\[
\lim_{N\rightarrow\infty}z_n = \delta_1/2.
\]


